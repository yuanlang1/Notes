# 浅层神经网络的实现

---

## 1. Neural Network Respresentation

2层神经网络，不同符号表示

![Neural Network Respresentation](images/2024-11-21-10-54-28.png)

---

## 2. Computing a Neural Network's output

计算隐藏层

![Neural Network Respresentation](images/2024-11-21-11-02-24.png)

输入x，用4个等式计算，向量化

![Neural Network Respresentation](images/2024-11-21-11-09-01.png)

---

## 3. Vectorizing across multiple examples（多样本向量化）

多个样本

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-15-42-10.png)

将m个样本横向堆叠，进行向量化运算

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-16-34-53.png)

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-16-45-33.png)

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-16-48-38.png)

---

## 4. Activation functions

sigmoid适用于二分类输出层，tanh使用于隐藏层，Relu,leaky Relu更快

![Activation functions](images/2024-11-21-22-51-02.png)

---

## 5. why need a nonlinear activation function?

使用线性激活函数相当于没有隐藏层

![ why need a nonlinear activation function?](images/2024-11-21-23-00-05.png)

---
