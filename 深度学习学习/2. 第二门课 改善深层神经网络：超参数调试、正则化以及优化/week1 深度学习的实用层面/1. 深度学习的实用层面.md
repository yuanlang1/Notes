# æ·±åº¦å­¦ä¹ çš„å®ç”¨å±‚é¢

- [æ·±åº¦å­¦ä¹ çš„å®ç”¨å±‚é¢](#æ·±åº¦å­¦ä¹ çš„å®ç”¨å±‚é¢)
  - [1. è®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•é›†ï¼ˆTrain / Dev / Test setsï¼‰](#1-è®­ç»ƒéªŒè¯æµ‹è¯•é›†train--dev--test-sets)
  - [2. åå·®ï¼Œæ–¹å·®ï¼ˆBias /Varianceï¼‰](#2-åå·®æ–¹å·®bias-variance)
  - [3. Basic recipe for machine learning](#3-basic-recipe-for-machine-learning)
  - [3. æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰](#3-æ­£åˆ™åŒ–regularization)
    - [3.1 Logistic regression](#31-logistic-regression)
    - [3.2 Neural network](#32-neural-network)
    - [3.3 ä¸ºä»€ä¹ˆæ­£åˆ™åŒ–æœ‰åˆ©äºé¢„é˜²è¿‡æ‹Ÿåˆå‘¢ï¼Ÿï¼ˆWhy regularization reduces overfitting?ï¼‰](#33-ä¸ºä»€ä¹ˆæ­£åˆ™åŒ–æœ‰åˆ©äºé¢„é˜²è¿‡æ‹Ÿåˆå‘¢why-regularization-reduces-overfitting)
    - [3.4 dropout æ­£åˆ™åŒ–ï¼ˆDropout Regularizationï¼‰](#34-dropout-æ­£åˆ™åŒ–dropout-regularization)
    - [3.5 ç†è§£ dropoutï¼ˆUnderstanding Dropoutï¼‰](#35-ç†è§£-dropoutunderstanding-dropout)
    - [3.6 å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆOther regularization methodsï¼‰](#36-å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•other-regularization-methods)
      - [3.6.1 Data augmentationï¼ˆæ•°æ®å¢å¼ºï¼‰](#361-data-augmentationæ•°æ®å¢å¼º)
      - [3.6.2 Early stopping](#362-early-stopping)
  - [4. åŠ å¿«è®­ç»ƒé€Ÿåº¦](#4-åŠ å¿«è®­ç»ƒé€Ÿåº¦)
    - [4.1 æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰è¾“å…¥ï¼ˆNormalizing inputsï¼‰](#41-æ ‡å‡†åŒ–å½’ä¸€åŒ–è¾“å…¥normalizing-inputs)
    - [4.2 æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸ï¼ˆVanishing / Exploding gradientsï¼‰](#42-æ¢¯åº¦æ¶ˆå¤±æ¢¯åº¦çˆ†ç‚¸vanishing--exploding-gradients)
    - [4.3 ç¥ç»ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–ï¼ˆWeight Initialization for Deep NetworksVanishing /Exploding gradientsï¼‰](#43-ç¥ç»ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–weight-initialization-for-deep-networksvanishing-exploding-gradients)
      - [4.3.1 Single neuron example](#431-single-neuron-example)
  - [5. æ¢¯åº¦æ£€éªŒï¼ˆGradient checkingï¼‰](#5-æ¢¯åº¦æ£€éªŒgradient-checking)
    - [5.1 æ¢¯åº¦æ£€éªŒï¼ˆGradient checkingï¼‰](#51-æ¢¯åº¦æ£€éªŒgradient-checking)
    - [5.2 æ¢¯åº¦æ£€éªŒåº”ç”¨çš„æ³¨æ„äº‹é¡¹ï¼ˆGradient Checking Implementation Notesï¼‰](#52-æ¢¯åº¦æ£€éªŒåº”ç”¨çš„æ³¨æ„äº‹é¡¹gradient-checking-implementation-notes)

---

## 1. è®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•é›†ï¼ˆTrain / Dev / Test setsï¼‰

è®­ç»ƒé›†ä¸€èˆ¬è¾ƒå¤§

![è®­ç»ƒï¼ŒéªŒè¯ï¼Œæµ‹è¯•é›†](images/2024-11-25-15-35-55.png)

---

## 2. åå·®ï¼Œæ–¹å·®ï¼ˆBias /Varianceï¼‰

![åå·®ï¼Œæ–¹å·®ï¼ˆBias /Varianceï¼‰](images/2024-11-25-16-34-35.png)

![åå·®ï¼Œæ–¹å·®ï¼ˆBias /Varianceï¼‰](images/2024-11-25-16-42-07.png)

---

## 3. Basic recipe for machine learning

![Basic recipe for machine learning](images/2024-11-25-16-48-10.png)

---

## 3. æ­£åˆ™åŒ–ï¼ˆRegularizationï¼‰

é˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå‡å°‘ç½‘ç»œè¯¯å·®

### 3.1 Logistic regression

é€»è¾‘å›å½’ä¸­ä½¿ç”¨åˆ°æ­£åˆ™åŒ–ï¼Œå…¶ä¸­è®²åˆ°L1æ­£åˆ™åŒ–ï¼ŒL2æ­£åˆ™åŒ–

![Logistic regression](images/2024-11-25-16-54-12.png)

L1æ­£åˆ™åŒ–ï¼š
$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}$$
To:
$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j |W_{k,j}^{[l]}|}_\text{L2 regularization cost} \tag{2}$$

L1åå‘ä¼ æ’­ï¼š
$$
\frac{d}{dW} \left( \frac{\lambda}{m} |W| \right) =
\begin{cases} 
\frac{\lambda}{m}, & W > 0 \\
-\frac{\lambda}{m}, & W < 0 \\
0, & W = 0
\end{cases}
$$
$$dW += (lambda / m) * np.sign(W)$$

L2æ­£åˆ™åŒ–:
$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}$$
To:
$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j (W_{k,j}^{[l]2}) }_\text{L2 regularization cost} \tag{2}$$

L2åå‘ä¼ æ’­ï¼š
$$\frac{d}{dW} ( \frac{1}{2}\frac{\lambda}{m}  W^2) = \frac{\lambda}{m} W$$
$$dW += (lambda / m) * W$$

### 3.2 Neural network

ç¥ç»ç½‘ç»œçš„çš„L2æ­£åˆ™åŒ–åˆç§°Frobenius norm

æ­£åˆ™åŒ–åˆç§°æƒé‡è¡°å‡

![Neural network](images/2024-11-26-10-05-50.png)

### 3.3 ä¸ºä»€ä¹ˆæ­£åˆ™åŒ–æœ‰åˆ©äºé¢„é˜²è¿‡æ‹Ÿåˆå‘¢ï¼Ÿï¼ˆWhy regularization reduces overfitting?ï¼‰

æ­£åˆ™åŒ–å‚æ•°è¶Šå¤§ï¼Œå¯¹æƒé‡çš„å‰Šå‡è¶Šå¤§ï¼Œä»è€Œé˜²æ­¢äº†è¿‡æ‹Ÿåˆ

![Why regularization reduces overfitting?](images/2024-11-26-10-11-07.png)

å½“$\lambda $å˜å¤§ï¼Œ$w^{[l]}$å°±ä¼šå‡å°ï¼Œ$z^{[l]} = w{[l]}a^{[l-1]} + b^{[l]}$ï¼Œ$w^{[l]}$å¾ˆå°ï¼Œå³$Z^{[l]}$å¾ˆå°ï¼Œ$g(z)$å°±ä¸ºçº¿æ€§ï¼Œå³æ¯ä¸€å±‚éƒ½ä¸ºçº¿æ€§å˜åŒ–ï¼Œæ•´ä¸ªç¥ç»ç½‘ç»œå°±ä¸ºçº¿æ€§ï¼Œå³é˜²æ­¢è¿‡æ‹Ÿåˆ

![Why regularization reduces overfitting?](images/2024-11-26-10-16-22.png)

### 3.4 dropout æ­£åˆ™åŒ–ï¼ˆDropout Regularizationï¼‰

å¸¸è§çš„æ­£åˆ™åŒ–é™¤äº†L1ï¼ŒL2æ­£åˆ™åŒ–è¿˜æœ‰dropoutæ­£åˆ™åŒ–

dropout æ­£åˆ™åŒ–æ˜¯æ¯ä¸€å±‚éšæœºè®¾ç½®æ¦‚ç‡ï¼Œå‡å°æŸäº›èŠ‚ç‚¹çš„æƒé‡ï¼Œåˆ é™¤æŸäº›èŠ‚ç‚¹

![dropout æ­£åˆ™åŒ–](images/2024-11-26-10-25-30.png)

é€šè¿‡keep-probæ¥è·å¾—éšæœºçŸ©é˜µï¼Œé€šè¿‡inverted dropoutï¼ˆåå‘éšæœºå¤±æ´»ï¼‰æ¥å®ç°dropoutæ­£åˆ™åŒ–

```python
å‰å‘ä¼ æ’­ï¼š
æ­¥éª¤ï¼š
# Step 1: initialize matrix D1 = np.random.rand(..., ...)
D1 = np.random.rand(*A1.shape)
# Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)  
D1 = D1 < keep_prob
# Step 3: shut down some neurons of A1  
A1 = A1 * D1 
 # Step 4: scale the value of neurons that haven't been shut down
A1 = A1 / keep_prob
Z2 = np.dot(W2, A1) + b2

åå‘ä¼ æ’­ï¼š
æ­¥éª¤ï¼š
dA1 = np.dot(W2.T, dZ2)
# Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation
dA1 = dA1 * D1                   
# Step 2: Scale the value of neurons that haven't been shut down
dA1 = dA1 / keep_prob
dZ1 = np.multiply(dA1, np.int64(A1 > 0))
dW1 = 1./m * np.dot(dZ1, X.T)
db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)      
```

![dropout æ­£åˆ™åŒ–](images/2024-11-26-10-31-56.png)

è®­ç»ƒçš„æ—¶å€™ç”¨ï¼Œæµ‹è¯•çš„æ—¶å€™ä¸è¦ç”¨
åœ¨æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­éƒ½è¦ç”¨

### 3.5 ç†è§£ dropoutï¼ˆUnderstanding Dropoutï¼‰

é€‚ç”¨äºè®¡ç®—æœºè§†è§‰

ç¼ºç‚¹ï¼šæ¯æ¬¡è¿­ä»£éƒ½ä¼šéšæœºç§»é™¤ä¸€äº›èŠ‚ç‚¹ï¼Œå¯¼è‡´ä»£ä»·å‡½æ•°Jä¸å†è¢«æ˜ç¡®å®šä¹‰ï¼Œå¾ˆéš¾ç¡®å®šæ˜¯å¦ä¸‹é™

![ç†è§£ dropout](images/2024-11-26-10-43-36.png)

### 3.6 å…¶ä»–æ­£åˆ™åŒ–æ–¹æ³•ï¼ˆOther regularization methodsï¼‰

#### 3.6.1 Data augmentationï¼ˆæ•°æ®å¢å¼ºï¼‰

å›¾ç‰‡ç¿»è½¬ï¼Œéšæœºè£å‰ªç­‰

![Data augmentation](images/2024-11-26-10-46-37.png)

#### 3.6.2 Early stopping

æå‰ç»“æŸè¿­ä»£ï¼Œç¼ºç‚¹æ˜¯æ— æ³•åŒæ—¶è¿›è¡Œä¼˜åŒ–Jå’Œé˜²æ­¢è¿‡æ‹Ÿåˆ

![Early stopping](images/2024-11-26-10-53-04.png)

---

## 4. åŠ å¿«è®­ç»ƒé€Ÿåº¦

### 4.1 æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰è¾“å…¥ï¼ˆNormalizing inputsï¼‰

- ä¸¤æ­¥ï¼š
  - subtract mean(é›¶å‡å€¼åŒ–)
  - normal variance(å½’ä¸€åŒ–æ–¹å·®)
  
![æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰è¾“å…¥](images/2024-11-26-10-57-42.png)

![æ ‡å‡†åŒ–ï¼ˆå½’ä¸€åŒ–ï¼‰è¾“å…¥](images/2024-11-26-11-00-55.png)

### 4.2 æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸ï¼ˆVanishing / Exploding gradientsï¼‰

æ¢¯åº¦æŒ‡æ•°çº§å‡å°æˆ–åŠ©é•¿ï¼Œä»–ä»¬çš„å€¼å°±ä¼šå¾ˆå°æˆ–å¾ˆå¤§ï¼Œå¯¼è‡´è®­ç»ƒå›°éš¾ï¼Œæ¢¯åº¦ä¸‹é™çš„æ­¥é•¿ä¼šå¾ˆå°

![æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸](images/2024-11-26-11-19-59.png)

### 4.3 ç¥ç»ç½‘ç»œçš„æƒé‡åˆå§‹åŒ–ï¼ˆWeight Initialization for Deep NetworksVanishing /Exploding gradientsï¼‰

ç”¨äºè§£å†³æ¢¯åº¦æ¶ˆå¤±/æ¢¯åº¦çˆ†ç‚¸

#### 4.3.1 Single neuron example

å…¶ä¸­è®²åˆ°ä¸åŒçš„å±‚çš„æ¿€æ´»å‡½æ•°ä½¿ç”¨åˆ°çš„ä¸åŒåˆå§‹æƒé‡çŸ©é˜µçš„åˆå§‹åŒ–ï¼ŒåŒ…å«tanh, Relu

![Single neuron example](images/2024-11-26-11-27-32.png)

---

## 5. æ¢¯åº¦æ£€éªŒï¼ˆGradient checkingï¼‰

### 5.1 æ¢¯åº¦æ£€éªŒï¼ˆGradient checkingï¼‰

![æ¢¯åº¦æ£€éªŒï¼ˆGradient checkingï¼‰](images/2024-11-26-20-49-40.png)

å¯ä»¥é€šè¿‡æ¢¯åº¦æ£€éªŒå¾—åˆ°çš„å€¼çš„å¤§å°æŸ¥çœ‹æ˜¯å¦ç¥ç»ç½‘ç»œæœ‰æ— bug

![æ¢¯åº¦æ£€éªŒï¼ˆGradient checkingï¼‰](images/2024-11-26-20-54-21.png)

- To compute `J_plus[i]`:
    1. Set $\theta^{+}$ to `np.copy(parameters_values)`
    2. Set $\theta^{+}_i$ to $\theta^{+}_i + \varepsilon$
    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\theta^{+}$ `))`.     
- To compute `J_minus[i]`: do the same thing with $\theta^{-}$
- æ€»çš„æ¥è¯´ï¼šè¾“å…¥å‚æ•°å‘é‡$ğœƒ$å’Œå°çš„æ‰°åŠ¨$ğœ€$
    1. $\theta^{+} = \theta + \varepsilon$
    2. $\theta^{-} = \theta - \varepsilon$
    3. $J^{+} = J(\theta^{+})$
    4. $J^{-} = J(\theta^{-})$
    5. $gradapprox = \frac{J^{+} - J^{-}}{2  \varepsilon}$
- è®¡ç®—$grad$
- è®¡ç®—$difference$
  - $$ difference = \frac {\mid\mid grad - gradapprox \mid\mid_2}{\mid\mid grad \mid\mid_2 + \mid\mid gradapprox \mid\mid_2}$$

### 5.2 æ¢¯åº¦æ£€éªŒåº”ç”¨çš„æ³¨æ„äº‹é¡¹ï¼ˆGradient Checking Implementation Notesï¼‰

![æ¢¯åº¦æ£€éªŒåº”ç”¨çš„æ³¨æ„äº‹é¡¹ï¼ˆGradient Checking Implementation Notesï¼‰](images/2024-11-26-21-00-14.png)

---
