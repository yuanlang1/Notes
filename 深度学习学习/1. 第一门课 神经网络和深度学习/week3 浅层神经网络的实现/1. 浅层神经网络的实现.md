# 浅层神经网络的实现

- [浅层神经网络的实现](#浅层神经网络的实现)
  - [1. Neural Network Respresentation](#1-neural-network-respresentation)
  - [2. Computing a Neural Network's output](#2-computing-a-neural-networks-output)
  - [3. Vectorizing across multiple examples（多样本向量化）](#3-vectorizing-across-multiple-examples多样本向量化)
  - [4. Activation functions](#4-activation-functions)
  - [5. why need a nonlinear activation function?](#5-why-need-a-nonlinear-activation-function)
  - [6. Derivatives of activation functions](#6-derivatives-of-activation-functions)
    - [6.1 sigmoid activation function](#61-sigmoid-activation-function)
    - [6.2 Tanh activation function](#62-tanh-activation-function)
    - [6.3 ReLU and leaky ReLU activation function](#63-relu-and-leaky-relu-activation-function)
  - [7. Gradient descent for Neural Network](#7-gradient-descent-for-neural-network)
    - [7.1 前向传播和反向传播的方程](#71-前向传播和反向传播的方程)
    - [7.2 推出反向传播的方程](#72-推出反向传播的方程)
  - [8. Random initialize](#8-random-initialize)


---

## 1. Neural Network Respresentation

2层神经网络，不同符号表示

![Neural Network Respresentation](images/2024-11-21-10-54-28.png)

---

## 2. Computing a Neural Network's output

计算隐藏层

![Neural Network Respresentation](images/2024-11-21-11-02-24.png)

输入x，用4个等式计算，向量化

![Neural Network Respresentation](images/2024-11-21-11-09-01.png)

---

## 3. Vectorizing across multiple examples（多样本向量化）

多个样本

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-15-42-10.png)

将m个样本横向堆叠，进行向量化运算

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-16-34-53.png)

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-16-45-33.png)

![Vectorizing across multiple examples（多样本向量化）](images/2024-11-21-16-48-38.png)

---

## 4. Activation functions

sigmoid适用于二分类输出层，tanh使用于隐藏层，Relu,leaky Relu更快

![Activation functions](images/2024-11-21-22-51-02.png)

---

## 5. why need a nonlinear activation function?

使用线性激活函数相当于没有隐藏层

![ why need a nonlinear activation function?](images/2024-11-21-23-00-05.png)

---

## 6. Derivatives of activation functions

### 6.1 sigmoid activation function

![sigmoid activation function](images/2024-11-22-10-39-48.png)

### 6.2 Tanh activation function

![Tanh activation function](images/2024-11-22-10-41-53.png)

### 6.3 ReLU and leaky ReLU activation function

![ReLU and leaky ReLU activation function](images/2024-11-22-10-44-16.png)

---

## 7. Gradient descent for Neural Network

### 7.1 前向传播和反向传播的方程

![Gradient descent for Neural Network](images/2024-11-22-10-49-07.png)

前向传播4个方程，反向传播6个方程

![Gradient descent for Neural Network](images/2024-11-22-11-09-21.png)

### 7.2 推出反向传播的方程

Logistic regression 反向传播的方程

![Logistic regression 反向传播的方程](images/2024-11-22-11-15-08.png)

$dz = a- y $
$dw = dz \cdot x$
$db = dz$

两层神经网络要算两步

![两层神经网络要算两步](images/2024-11-22-15-12-40.png)

Summary of gradient descent

右边为多样本

![右边为多样本](images/2024-11-22-15-23-16.png)

![Summary of gradient descent](images/2024-11-22-15-22-25.png)

---

## 8. Random initialize

如果刚开始w[1]，b[1]都为0，那隐藏层单元计算一样且变化一样

初始化参数一般都很小

![初始化参数](images/2024-11-22-15-32-59.png)

---
